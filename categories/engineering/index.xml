<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Engineering on 自由の灵魂</title><link>https://ctliu3.xyz/categories/engineering/</link><description>Recent content in Engineering on 自由の灵魂</description><generator>Hugo -- gohugo.io</generator><language>zh</language><lastBuildDate>Sun, 16 Apr 2017 09:19:22 +0000</lastBuildDate><atom:link href="https://ctliu3.xyz/categories/engineering/index.xml" rel="self" type="application/rss+xml"/><item><title>Implement style transfer in Pytorch</title><link>https://ctliu3.xyz/posts/implement-style-transfer-in-pytorch/</link><pubDate>Sun, 16 Apr 2017 09:19:22 +0000</pubDate><guid>https://ctliu3.xyz/posts/implement-style-transfer-in-pytorch/</guid><description>&lt;p>Prisma 在去年异常火爆，它能将艺术图片（如梵高的「星空」）与任意图片融合，生成一张同时带有两者风格的图片。专业点的说法，这叫 Style transfer（风格迁移）。其实第一篇风格化相关的文章 2015 年就有了，但由于它把训练和预测放一起了，导致无法做到实时。当然，这个问题后面几个月就被解决了，也就有了再后来的 Prisma。在这篇文章里，只介绍 style transfer 的主要思路，Pytorch 实现的代码可以查看&lt;a href="https://github.com/ctliu3/neural-style">这里&lt;/a>。&lt;/p></description></item><item><title>Rate Limiter</title><link>https://ctliu3.xyz/posts/rate-limiter/</link><pubDate>Sat, 03 Dec 2016 10:52:19 +0000</pubDate><guid>https://ctliu3.xyz/posts/rate-limiter/</guid><description>&lt;p>I dig into rate limiter these days. Rate limiter is a tool that limits the number of request in a given short period. Any service only can handle limited requests with limited resources (cpu, memory, etc). Rate limiter prevents service to reach a high load that beyond its load capacity.&lt;/p></description></item><item><title>参数服务器</title><link>https://ctliu3.xyz/posts/parameter-server/</link><pubDate>Sat, 12 Nov 2016 15:20:32 +0000</pubDate><guid>https://ctliu3.xyz/posts/parameter-server/</guid><description>&lt;p>参数服务器（Parameter Sever）是大规模机器学习的一个部分，比如 dmlc 中的 &lt;a href="https://github.com/dmlc/ps-lite">ps-lite&lt;/a>, 比如微软的 &lt;a href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwiiqp-RiaPQAhUDsI8KHZnNDBMQFgggMAA&amp;amp;url=https%3A%2F%2Fpdfs.semanticscholar.org%2F043a%2Ffbd936c95d0e33c4a391365893bd4102f1a7.pdf&amp;amp;usg=AFQjCNFFnirPzFhPQQ4KTeDH5MX8ff1OEw&amp;amp;sig2=iQsdfQFyUcPhG8kkP9B0-g">project adam&lt;/a>。本质上来，参数服务器就是个分布式系统，用于更新机器学习的模型参数。因此，一个完善的参数服务器要考虑到错误容忍、可扩展性、数据一致性等。&lt;/p></description></item><item><title>Build a high performance image prediction service</title><link>https://ctliu3.xyz/posts/high-performance-image-prediction-service/</link><pubDate>Sun, 04 Sep 2016 22:36:00 +0000</pubDate><guid>https://ctliu3.xyz/posts/high-performance-image-prediction-service/</guid><description>&lt;p>Deep learning is absolutely one of the keywords in these years. Like many other company, we use deep learning to solve some tasks, like image classification, OCR, face detection, etc. I’ve spent some time building the image classification service and done some optimizations during my work. I will share some of my experience in this article.&lt;/p></description></item><item><title>Tips you should know about Caffe</title><link>https://ctliu3.xyz/posts/tips-you-should-know-about-caffe/</link><pubDate>Sat, 08 Aug 2015 16:49:18 +0000</pubDate><guid>https://ctliu3.xyz/posts/tips-you-should-know-about-caffe/</guid><description>&lt;p>&lt;a href="https://github.com/BVLC/caffe">Caffe&lt;/a> is a wonderful deep learning (DL) framework, written in C++ and it is still under construction. I use Caffe to do some image related tasks these days. Technologically, I use Caffe just around one week, so I’m also a newbie. I encounter some “pits” when using Caffe. So I write down some tips, hope they are helpful to you.&lt;/p></description></item><item><title>NLP PA: Parsing</title><link>https://ctliu3.xyz/posts/nlp-pa-parsing/</link><pubDate>Fri, 05 Apr 2013 13:57:15 +0000</pubDate><guid>https://ctliu3.xyz/posts/nlp-pa-parsing/</guid><description>&lt;p>本周的 PA 是对给定的句子求出相应的句法树（Parsing tree）。实际上，Parsing 经常也做为一些实际应用的预处理，比如机器翻译（Machine Translation, MT），因为不同语言的句子结构（如主谓宾）有很大的区别，通过句法分析得到句子的结构及单词词性，再根据不同语言本身的句法特点进行变换。&lt;/p></description></item><item><title>NLP PA: Hidden Markov Models</title><link>https://ctliu3.xyz/posts/nlp-pa-hidden-markov-models/</link><pubDate>Sun, 24 Mar 2013 13:27:49 +0000</pubDate><guid>https://ctliu3.xyz/posts/nlp-pa-hidden-markov-models/</guid><description>&lt;p>&lt;a href="https://class.coursera.org/nlangp-001/class/index" title="NLP">传送门&lt;/a>&lt;/p>
&lt;p>HMM 是通过预测一连串的 $y_i$ 来得到模型 $p(x_1 \dots x_n, y_1 \dots y_n)$ 的概率最大值。本周的 Programming 主要是利用 HMM 来做针对句子的词性 tagger。&lt;/p></description></item></channel></rss>