<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><style>:root{--accent-color:#FF4D4D}</style><title>Implement style transfer in Pytorch</title>
<meta name=description content="Moments of Life"><meta name=keywords content="Pytorch,Deep Learning,Computer Vision"><meta property="og:url" content="https://ctliu3.xyz/posts/implement-style-transfer-in-pytorch/"><meta property="og:type" content="website"><meta property="og:title" content="Implement style transfer in Pytorch"><meta property="og:description" content="Moments of Life"><meta property="og:image" content="https://ctliu3.xyz/avatar.png"><meta property="og:image:secure_url" content="https://ctliu3.xyz/avatar.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Implement style transfer in Pytorch"><meta name=twitter:description content="Moments of Life"><meta property="twitter:domain" content="https://ctliu3.xyz/posts/implement-style-transfer-in-pytorch/"><meta property="twitter:url" content="https://ctliu3.xyz/posts/implement-style-transfer-in-pytorch/"><meta name=twitter:image content="https://ctliu3.xyz/avatar.png"><link rel=canonical href=https://ctliu3.xyz/posts/implement-style-transfer-in-pytorch/><link rel=stylesheet type=text/css href=/css/normalize.min.css media=print><link rel=stylesheet type=text/css href=/css/main.min.css><link id=dark-theme rel=stylesheet href=/css/dark.min.css><script src=/js/bundle.min.edd985581bf860dfb4507e5885197f1680160c7fe19f23b31e183126d99dd596.js integrity="sha256-7dmFWBv4YN+0UH5YhRl/FoAWDH/hnyOzHhgxJtmd1ZY="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css integrity=sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js integrity=sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script></head><body><script type=text/javascript>setThemeByUserPref()</script><header class=header><nav class=header-nav><div class=avatar><a href=https://ctliu3.xyz/><img src=/avatar.png alt=avatar></a></div><div class=nav-title><a class=nav-brand href=https://ctliu3.xyz/>自由の灵魂</a></div><div class=nav-links><div class=nav-link><a href=https://ctliu3.xyz/><span data-feather=home></span> Home</a></div><div class=nav-link><a href=https://ctliu3.xyz/posts/><span data-feather=book></span> Posts</a></div><div class=nav-link><a href=https://ctliu3.xyz/about><span data-feather=user></span> About</a></div><span class=nav-icons-divider></span><div class="nav-link dark-theme-toggle"><span id=dark-theme-toggle-screen-reader-target class=sr-only></span>
<a><span id=theme-toggle-icon data-feather=moon></span></a></div><div class=nav-link id=hamburger-menu-toggle><span id=hamburger-menu-toggle-screen-reader-target class=sr-only>menu</span>
<a><span data-feather=menu></span></a></div><ul class="nav-hamburger-list visibility-hidden"><li class=nav-item><a href=https://ctliu3.xyz/><span data-feather=home></span> Home</a></li><li class=nav-item><a href=https://ctliu3.xyz/posts/><span data-feather=book></span> Posts</a></li><li class=nav-item><a href=https://ctliu3.xyz/about><span data-feather=user></span> About</a></li><li class="nav-item dark-theme-toggle"><span id=dark-theme-toggle-screen-reader-target class=sr-only>theme</span>
<a><span id=theme-toggle-icon data-feather=moon></span></a></li></ul></div></nav></header><main id=content><div class="post container"><div class=post-header-section><h1>Implement style transfer in Pytorch</h1><small role=doc-subtitle></small><p class=post-date>四月 16, 2017</p><ul class=post-tags><li class=post-tag><a href=https://ctliu3.xyz/tags/pytorch>Pytorch</a></li><li class=post-tag><a href=https://ctliu3.xyz/tags/deep-learning>Deep Learning</a></li><li class=post-tag><a href=https://ctliu3.xyz/tags/computer-vision>Computer Vision</a></li></ul></div><div class=post-content><p><p>Prisma 在去年异常火爆，它能将艺术图片（如梵高的「星空」）与任意图片融合，生成一张同时带有两者风格的图片。专业点的说法，这叫 Style transfer（风格迁移）。其实第一篇风格化相关的文章 2015 年就有了，但由于它把训练和预测放一起了，导致无法做到实时。当然，这个问题后面几个月就被解决了，也就有了再后来的 Prisma。在这篇文章里，只介绍 style transfer 的主要思路，Pytorch 实现的代码可以查看<a href=https://github.com/ctliu3/neural-style>这里</a>。</p><h1 id=style-transfer>Style Transfer</h1><p>对于一个图像分类网络，输入是图像，输出是该图像属于每个类别的概率。而训练风格迁移，输入是艺术图和待风格化的图，输出是两者的融合，即风格化后的图。虽然我拿它跟图像分类作对比，但除了都使用 CNN 来提取特征，它们并不具有可比性。具体流程可以看下面的流程图</p><figure><img src=/Implement-style-transfer-in-Pytorch/neural-style-flow.jpg alt=风格迁移流程图><figcaption><p>风格迁移流程图</p></figcaption></figure><ul><li>风络迁移网络[1]使用中间层的 feature map 来计算 loss。如果想让保留更多原图的信息，靠近输入的层可以设置较高的权重。论文中使用的是 <code>vgg19</code>，全连接层不需要用到。</li><li>使用的网络是在 imagenet 等数据集上预训练好的，网络的参数在风格化训练过程中并不更新，而是更新上图中的输入 <code>X</code>. <code>X</code> 是一幅初始图片，每一次反向传播都会更改 <code>X</code> 的像素值。为了让训练更快、更好地逼进解，可以使用待风格化的图片作为初始图片。这种利用 image graident 来更新原图的方式也可以用来生成 <a href=http://karpathy.github.io/2015/03/30/breaking-convnets/>fool image</a>。</li><li>Loss 的计算上，<code>X</code> 与艺术图和原图的 loss 上有些区别。我没实验过，但只与原图的高层特征求 loss，应该是为了让结果更接近艺术图。加上如果使用原始图片作为初始 <code>X</code> 的话，只对高层特征求 loss 确实是合理的。</li></ul><h1 id=make-it-faster>Make it faster</h1><p>每个图像都要经过多轮迭代才能得到风格化的图片，这种方法显然实用性不强。为了解决这个问题，[2] 修改了原来的训练流程：使用两个网络，一个网络用来学习风格化的参数，我们把它称为 <code>image transform net</code>，一个网络用来计算 loss，我们称之为 <code>loss net</code>。<code>loss net</code> 主要用来计算损失，因此不更新参数。<code>image transform net</code> 的输出即是最后风格化后的图。[2] 使用了如下的网络结构</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ImageTransformNet</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>bn1(self<span style=color:#f92672>.</span>conv1(x)))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>bn2(self<span style=color:#f92672>.</span>conv2(x)))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>bn3(self<span style=color:#f92672>.</span>conv3(x)))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>res1(x) <span style=color:#75715e># residual block</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>res2(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>res3(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>res4(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>res5(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>bn4(self<span style=color:#f92672>.</span>conv4(x)))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>relu(self<span style=color:#f92672>.</span>bn5(self<span style=color:#f92672>.</span>conv5(x)))
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv6(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><h1 id=experience-with-pytorch>Experience with Pytorch</h1><p>其实写这个项目的主要一个原因是为了熟悉 Pytorch。这很大程度源于看了 Andrej Karpathy 的一句<a href=https://twitter.com/PyTorch/status/829539819709624321/photo/1>推文</a>。
为了吸引更多的人使用，不管是 pytorch 还是 Caffe, MXNet 等框架都在极力降低入坑的门槛，更简洁的 API，更丰富的预训练好的模型。以致发展到现在，各家的接口，在某种程度上说，长得都差不多，特别是 python 的接口。因此到最后，还是回归到架构的设计上，哪个更为灵活，使用的显存更少，更好的支持分布式训练等等。从这段时间的使用情况来看，pytorch</p><ul><li>社区并不活跃，<a href=https://discuss.pytorch.org/>discuss</a> 上的帖子并不多，回复得比较积极的似乎总是那几个人。</li><li>接口非常易用，文档相比于 Caffe, MXNet 也算完善，但离 Tensorflow 还有些距离。不过一般的问题跟跟源码或是在论坛上基本都能找到解答。</li><li>官方主打的动态图模型。我没在其它框架上写过定制化特别强的复杂网络，因此不好比较。在使用 pytorch 过程中，我确实感觉到了一些便利：定义新的 layer/loss，在 forward 过程中随意的把中间结果拿出来，放在其他网络中。</li></ul><h1 id=reference>Reference</h1><p><a href=http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf>[1]</a>: Gatys L A, Ecker A S, Bethge M, Image Style Transfer Using Convolutional Neural Networks, CVPR 2016</p><p><a href=https://arxiv.org/abs/1603.08155>[2]</a>: Johnson J, Alahi A, Fei-Fei L., Perceptual Losses for Real-Time Style Transfer and Super-Resolution, ECCV 2016</p></p></div><div class=prev-next></div><svg id="btt-button" class="arrow-logo" xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 384 512" onclick="topFunction()" title="Go to top"><path d="M177 159.7l136 136c9.4 9.4 9.4 24.6.0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9.0L160 255.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9.0L7 329.7c-9.4-9.4-9.4-24.6.0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1z"/></svg><script>let backToTopButton=document.getElementById("btt-button");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?backToTopButton.style.display="block":backToTopButton.style.display="none"}function topFunction(){smoothScrollToTop()}function smoothScrollToTop(){const e=()=>{const t=document.documentElement.scrollTop||document.body.scrollTop;t>0&&(window.requestAnimationFrame(e),window.scrollTo(0,t-t/8))};e()}</script></div><aside class=post-toc><nav id=toc><nav id=TableOfContents><ul><li><a href=#style-transfer>Style Transfer</a></li><li><a href=#make-it-faster>Make it faster</a></li><li><a href=#experience-with-pytorch>Experience with Pytorch</a></li><li><a href=#reference>Reference</a></li></ul></nav></nav></aside></main><footer class=footer><span>&copy; 2023 ctliu3</span>
<span>Made with &#10084;&#65039; using <a target=_blank href=https://github.com/526avijitgupta/gokarna>Gokarna</a></span></footer></body></html>