<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><style>:root{--accent-color:#FF4D4D}</style><title>Build a high performance image prediction service</title>
<meta name=description content="Moments of Life"><meta name=keywords content><meta property="og:url" content="https://ctliu3.xyz/posts/high-performance-image-prediction-service/"><meta property="og:type" content="website"><meta property="og:title" content="Build a high performance image prediction service"><meta property="og:description" content="Moments of Life"><meta property="og:image" content="https://ctliu3.xyz/avatar.png"><meta property="og:image:secure_url" content="https://ctliu3.xyz/avatar.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Build a high performance image prediction service"><meta name=twitter:description content="Moments of Life"><meta property="twitter:domain" content="https://ctliu3.xyz/posts/high-performance-image-prediction-service/"><meta property="twitter:url" content="https://ctliu3.xyz/posts/high-performance-image-prediction-service/"><meta name=twitter:image content="https://ctliu3.xyz/avatar.png"><link rel=canonical href=https://ctliu3.xyz/posts/high-performance-image-prediction-service/><link rel=stylesheet type=text/css href=/css/normalize.min.css media=print><link rel=stylesheet type=text/css href=/css/main.min.css><link id=dark-theme rel=stylesheet href=/css/dark.min.css><script src=/js/bundle.min.edd985581bf860dfb4507e5885197f1680160c7fe19f23b31e183126d99dd596.js integrity="sha256-7dmFWBv4YN+0UH5YhRl/FoAWDH/hnyOzHhgxJtmd1ZY="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css integrity=sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js integrity=sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script></head><body><script type=text/javascript>setThemeByUserPref()</script><header class=header><nav class=header-nav><div class=avatar><a href=https://ctliu3.xyz/><img src=/avatar.png alt=avatar></a></div><div class=nav-title><a class=nav-brand href=https://ctliu3.xyz/>自由の灵魂</a></div><div class=nav-links><div class=nav-link><a href=https://ctliu3.xyz/><span data-feather=home></span> Home</a></div><div class=nav-link><a href=https://ctliu3.xyz/posts/><span data-feather=book></span> Posts</a></div><div class=nav-link><a href=https://ctliu3.xyz/about><span data-feather=user></span> About</a></div><span class=nav-icons-divider></span><div class="nav-link dark-theme-toggle"><span id=dark-theme-toggle-screen-reader-target class=sr-only></span>
<a><span id=theme-toggle-icon data-feather=moon></span></a></div><div class=nav-link id=hamburger-menu-toggle><span id=hamburger-menu-toggle-screen-reader-target class=sr-only>menu</span>
<a><span data-feather=menu></span></a></div><ul class="nav-hamburger-list visibility-hidden"><li class=nav-item><a href=https://ctliu3.xyz/><span data-feather=home></span> Home</a></li><li class=nav-item><a href=https://ctliu3.xyz/posts/><span data-feather=book></span> Posts</a></li><li class=nav-item><a href=https://ctliu3.xyz/about><span data-feather=user></span> About</a></li><li class="nav-item dark-theme-toggle"><span id=dark-theme-toggle-screen-reader-target class=sr-only>theme</span>
<a><span id=theme-toggle-icon data-feather=moon></span></a></li></ul></div></nav></header><main id=content><div class="post container"><div class=post-header-section><h1>Build a high performance image prediction service</h1><small role=doc-subtitle></small><p class=post-date>九月 4, 2016</p><ul class=post-tags></ul></div><div class=post-content><p><p>Deep learning is absolutely one of the keywords in these years. Like many other company, we use deep learning to solve some tasks, like image classification, OCR, face detection, etc. I’ve spent some time building the image classification service and done some optimizations during my work. I will share some of my experience in this article.</p><h1 id=cpu-orgpu>CPU or GPU</h1><p>It really depends on a variety of factors, such as the request scale, power consumption, etc. GPU is expensive, but running the forward phase on the GPU device has dozens of times speed-up than CPU even you use OpenBLAS or MKL to accelerate the underlying matrix manipulation in CPU.</p><h1 id=prediction-modeldesign>Prediction Model design</h1><p>In general case, people will try to predict the image category using convolutional neural networks (CNN), like AlexNet, GoogleNet, or VGGNet. Different networks has different prediction performance and time complexity in inference phase. For example, executing forward stage in AlexNet is 4 times faster than GoogleNet. But GoogleNet has better classification ability since it’s deeper and wider. Directly training the pre-trained model on the ImageNet or some other open database might not meet your need for precision and recall. To gain a higher precision and recall, there are many approaches worth to try: re-design the network, use the cascaded model, integrate the outputs of multi-network, predict with multiple scales. All these strategies are doable and sound great. But remember, these designs might more or less increase the time and GPU memory cost. So, take the time and space complexity into account while designing the neural network models.</p><h1 id=prediction-frameworks>Prediction Frameworks</h1><p>Building an neural network prediction framework from scratch of you own might not a smart idea since there are some well-built projects such as Caffe, MXNet and TensorFlow, which can be deployed directly in the production environment. Take following pros and cons into account before choosing one.</p><ul><li>Caffe. Maybe this is the most popular framework that people are familiar with, which means you can find lots of solutions for the problem you encounter on the Internet. Caffe has graceful C++ and python API. The cons is that it occupies too much GPU memory when comparing with MXNet and TensorFlow. This is the design issue. Caffe will allocate the memory for parameters and output of each layer and these memory can not be reused. For GoogleNet model, Caffe allocates 6x GPU memory than MXNet. The good news is that if you want to load multiple identical models on the same process, you can share the parameters of each layer.</li><li>MXNet. A deep learning that has a better design of GPU memory allocation. It constructs the neural network as a computation graph. Each node in the graph can be reuse and it has high performance since the computation is not executed layer-by-layer, but based on the node (a node is a computation unit like add, minus, multiply and divide). Concretely, a node can be executed if its preceding nodes are finished. Like Caffe, MXNet also has graceful C++ and python API. The bad news is MXNet has worse performance when running on CPU. Take a look of this issue.</li><li>TensorFlow. The GPU memory usage and performance are close to MXNet. I don’t have experience on TensorFlow on production environment. The official team provides a project, called serving, to help deploy deep learning models.</li></ul><h1 id=single-or-batch-prediction>Single or Batch prediction</h1><p>Running a GoogleNet V1 in forward (with cuDNN) with the batch size of 10 is only~30% time of running batch size of 1 in 10 times. This conclusion provides us a way to improve the service performance. We can add a middleware to serve the request from client and forward them to prediction service. There are some features for the middleware showing as followings:</p><ul><li>Request coalescing duration. The middleware needs to forward the batched images within a limited period of time even the requested imaged to be predicted is smaller than the batch size. If not, it brings high-latency. </li><li>Batch size. In general, the throughput of the prediction system will be enhanced if batch size increased. But too larger a batch size leads to high GPU memory occupation and service latency.</li><li>Rate limiting. The middleware can limit the number of request to fit the service limitation of prediction service.</li></ul><p>Following figure a flexible and extensible architecture.</p><figure><img src=/high-performance-image-prediction-service/prediction-flow.png alt=prediction-flow><figcaption><p>prediction-flow</p></figcaption></figure><h1 id=image-decode-optimization>Image decode optimization</h1><p>Decode a 1080P image will cost ~30ms in CPU while running inference of GoogleNet V1 only cost ~10ms in GPU. Thus, decoding image may somehow become bottleneck. To solve this, It’s better to resize the image in front end. In addition, decoding image with in libjpeg-turbo library ~30% faster than libjpeg (I got this result but the official site claims that better performance can be achieved).</p><h1 id=conclusion>Conclusion</h1><p>The strategies above mentioned are not only suitable for image prediction service, but also for face recognition, face detection, etc.</p></p></div><div class=prev-next></div><svg id="btt-button" class="arrow-logo" xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 384 512" onclick="topFunction()" title="Go to top"><path d="M177 159.7l136 136c9.4 9.4 9.4 24.6.0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9.0L160 255.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9.0L7 329.7c-9.4-9.4-9.4-24.6.0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1z"/></svg><script>let backToTopButton=document.getElementById("btt-button");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?backToTopButton.style.display="block":backToTopButton.style.display="none"}function topFunction(){smoothScrollToTop()}function smoothScrollToTop(){const e=()=>{const t=document.documentElement.scrollTop||document.body.scrollTop;t>0&&(window.requestAnimationFrame(e),window.scrollTo(0,t-t/8))};e()}</script></div><aside class=post-toc><nav id=toc><nav id=TableOfContents><ul><li><a href=#cpu-orgpu>CPU or GPU</a></li><li><a href=#prediction-modeldesign>Prediction Model design</a></li><li><a href=#prediction-frameworks>Prediction Frameworks</a></li><li><a href=#single-or-batch-prediction>Single or Batch prediction</a></li><li><a href=#image-decode-optimization>Image decode optimization</a></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></nav></aside></main><footer class=footer><span>&copy; 2023 ctliu3</span>
<span>Made with &#10084;&#65039; using <a target=_blank href=https://github.com/526avijitgupta/gokarna>Gokarna</a></span></footer></body></html>