<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Space of ctliu3</title><link>http://example.org/posts/</link><description>Recent content in Posts on Space of ctliu3</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sun, 16 Apr 2017 09:19:22 +0000</lastBuildDate><atom:link href="http://example.org/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Implement style transfer in Pytorch</title><link>http://example.org/posts/implement-style-transfer-in-pytorch/</link><pubDate>Sun, 16 Apr 2017 09:19:22 +0000</pubDate><guid>http://example.org/posts/implement-style-transfer-in-pytorch/</guid><description>&lt;p>Prisma 在去年异常火爆，它能将艺术图片（如梵高的「星空」）与任意图片融合，生成一张同时带有两者风格的图片。专业点的说法，这叫 Style transfer（风格迁移）。其实第一篇风格化相关的文章 2015 年就有了，但由于它把训练和预测放一起了，导致无法做到实时。当然，这个问题后面几个月就被解决了，也就有了再后来的 Prisma。在这篇文章里，只介绍 style transfer 的主要思路，Pytorch 实现的代码可以查看&lt;a href="https://github.com/ctliu3/neural-style">这里&lt;/a>。&lt;/p></description></item><item><title>Rate Limiter</title><link>http://example.org/posts/rate-limiter/</link><pubDate>Sat, 03 Dec 2016 10:52:19 +0000</pubDate><guid>http://example.org/posts/rate-limiter/</guid><description>&lt;p>I dig into rate limiter these days. Rate limiter is a tool that limits the number of request in a given short period. Any service only can handle limited requests with limited resources (cpu, memory, etc). Rate limiter prevents service to reach a high load that beyond its load capacity.&lt;/p></description></item><item><title>参数服务器</title><link>http://example.org/posts/parameter-server/</link><pubDate>Sat, 12 Nov 2016 15:20:32 +0000</pubDate><guid>http://example.org/posts/parameter-server/</guid><description>&lt;p>参数服务器（Parameter Sever）是大规模机器学习的一个部分，比如 dmlc 中的 &lt;a href="https://github.com/dmlc/ps-lite">ps-lite&lt;/a>, 比如微软的 &lt;a href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwiiqp-RiaPQAhUDsI8KHZnNDBMQFgggMAA&amp;amp;url=https%3A%2F%2Fpdfs.semanticscholar.org%2F043a%2Ffbd936c95d0e33c4a391365893bd4102f1a7.pdf&amp;amp;usg=AFQjCNFFnirPzFhPQQ4KTeDH5MX8ff1OEw&amp;amp;sig2=iQsdfQFyUcPhG8kkP9B0-g">project adam&lt;/a>。本质上来，参数服务器就是个分布式系统，用于更新机器学习的模型参数。因此，一个完善的参数服务器要考虑到错误容忍、可扩展性、数据一致性等。&lt;/p></description></item><item><title>阿姆斯特丹之行</title><link>http://example.org/posts/days-in-amsterdam/</link><pubDate>Sun, 16 Oct 2016 23:23:37 +0000</pubDate><guid>http://example.org/posts/days-in-amsterdam/</guid><description>&lt;figure>&lt;img src="http://example.org/days-in-amsterdam/amsterdam-canal-view.jpg"
alt="阴天里的阿姆斯特丹运河"/>&lt;figcaption>
&lt;p>阴天里的阿姆斯特丹运河&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>当下火车，踏上阿姆斯特丹的街道时，我看到了以往在屏幕上才会看到的画面：不高但匠心独运的欧式建筑、漂浮着建筑倒影的运河、时停时飞的鸽子。才 8 点多，街上偶尔几个跑步的人，地上不算干净，空气中透着寒气，城市还没完全醒来。&lt;/p></description></item><item><title>日本之行</title><link>http://example.org/posts/japan-trip/</link><pubDate>Fri, 16 Sep 2016 18:20:31 +0000</pubDate><guid>http://example.org/posts/japan-trip/</guid><description>&lt;figure>&lt;img src="http://example.org/japan-trip/Ginza.JPG"
alt="夜晚的银座"/>&lt;figcaption>
&lt;p>夜晚的银座&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>不久前还计划着和友人 &lt;a href="https://www.facebook.com/zhichun.wu">wzc&lt;/a> 到日本玩几天，转眼间旅程已经结束，而我又回到弥漫沉闷气息的北京。了解一个地方，不外忽历史和人文。然而，短暂的旅程、蹩脚的交流能力和不够外放的性格，让我觉得和日本人的生活总隔着一层薄膜，以致于对日本的印象只有那些浮于表面的第一观感。&lt;/p></description></item><item><title>Build a high performance image prediction service</title><link>http://example.org/posts/high-performance-image-prediction-service/</link><pubDate>Sun, 04 Sep 2016 22:36:00 +0000</pubDate><guid>http://example.org/posts/high-performance-image-prediction-service/</guid><description>&lt;p>Deep learning is absolutely one of the keywords in these years. Like many other company, we use deep learning to solve some tasks, like image classification, OCR, face detection, etc. I’ve spent some time building the image classification service and done some optimizations during my work. I will share some of my experience in this article.&lt;/p></description></item><item><title>Tips you should know about Caffe</title><link>http://example.org/posts/tips-you-should-know-about-caffe/</link><pubDate>Sat, 08 Aug 2015 16:49:18 +0000</pubDate><guid>http://example.org/posts/tips-you-should-know-about-caffe/</guid><description>&lt;p>&lt;a href="https://github.com/BVLC/caffe">Caffe&lt;/a> is a wonderful deep learning (DL) framework, written in C++ and it is still under construction. I use Caffe to do some image related tasks these days. Technologically, I use Caffe just around one week, so I’m also a newbie. I encounter some “pits” when using Caffe. So I write down some tips, hope they are helpful to you.&lt;/p></description></item><item><title>NLP PA: Parsing</title><link>http://example.org/posts/nlp-pa-parsing/</link><pubDate>Fri, 05 Apr 2013 13:57:15 +0000</pubDate><guid>http://example.org/posts/nlp-pa-parsing/</guid><description>&lt;p>本周的 PA 是对给定的句子求出相应的句法树（Parsing tree）。实际上，Parsing 经常也做为一些实际应用的预处理，比如机器翻译（Machine Translation, MT），因为不同语言的句子结构（如主谓宾）有很大的区别，通过句法分析得到句子的结构及单词词性，再根据不同语言本身的句法特点进行变换。&lt;/p></description></item><item><title>NLP PA: Hidden Markov Models</title><link>http://example.org/posts/nlp-pa-hidden-markov-models/</link><pubDate>Sun, 24 Mar 2013 13:27:49 +0000</pubDate><guid>http://example.org/posts/nlp-pa-hidden-markov-models/</guid><description>&lt;p>&lt;a href="https://class.coursera.org/nlangp-001/class/index" title="NLP">传送门&lt;/a>&lt;/p>
&lt;p>HMM 是通过预测一连串的 $y_i$ 来得到模型 $p(x_1 \dots x_n, y_1 \dots y_n)$ 的概率最大值。本周的 Programming 主要是利用 HMM 来做针对句子的词性 tagger。&lt;/p></description></item></channel></rss>